{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "testnlp.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMfBGdHaePLAE2oyfQqhN0/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wudih146/PythonML/blob/main/testnlp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R7cIV2rxqMfW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b81ac23c-417b-4679-89c4-32ffedf44fee"
      },
      "source": [
        "import os \r\n",
        "os.getcwd()\r\n",
        "def load_dictionary():\r\n",
        "    dic = set()\r\n",
        "\r\n",
        "    # 按行读取字典文件，每行第一个空格之前的字符串提取出来。\r\n",
        "    for line in open(\"CoreNatureDictionary.mini.txt\",\"r\"):\r\n",
        "        dic.add(line[0:line.find('\t')])\r\n",
        "    \r\n",
        "    return dic\r\n",
        "dic=load_dictionary()\r\n",
        "mydic=list(dic)\r\n",
        "mydic[:100]"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['记叙',\n",
              " '责任书',\n",
              " '高官',\n",
              " '空管局',\n",
              " '平平淡淡',\n",
              " '校时钟',\n",
              " '试穿',\n",
              " '宦',\n",
              " '半世',\n",
              " '启',\n",
              " '夤',\n",
              " '燕南园',\n",
              " '端电压',\n",
              " '打头',\n",
              " '突地',\n",
              " '绍兴市',\n",
              " '缨',\n",
              " '目眩',\n",
              " '髯口',\n",
              " '敷料',\n",
              " '梆子',\n",
              " '树形',\n",
              " '无以',\n",
              " '挺胸',\n",
              " '秒',\n",
              " '记要',\n",
              " '向心',\n",
              " '非农业',\n",
              " '恢宏',\n",
              " '委以',\n",
              " '下家',\n",
              " '沙鸡',\n",
              " '灵光',\n",
              " '万方',\n",
              " '宁肯',\n",
              " '螟蛉',\n",
              " '缝缝补补',\n",
              " '京山县',\n",
              " '言之凿凿',\n",
              " '标的',\n",
              " '充要条件',\n",
              " '垸',\n",
              " '磨合期',\n",
              " '绷子',\n",
              " '莘',\n",
              " '女巫',\n",
              " '人造板',\n",
              " '底肥',\n",
              " '招待',\n",
              " '收支簿',\n",
              " '苦处',\n",
              " '轻机关枪',\n",
              " '式微',\n",
              " '翻译器',\n",
              " '不好',\n",
              " '冬运',\n",
              " '北方邦',\n",
              " '橐',\n",
              " '后撤',\n",
              " '薄父母',\n",
              " '白食',\n",
              " '鼓吹者',\n",
              " '骛',\n",
              " '座上宾',\n",
              " '纾',\n",
              " '入土',\n",
              " '分析语',\n",
              " '土模',\n",
              " '源源',\n",
              " '阻抗',\n",
              " '肘子',\n",
              " '醉态',\n",
              " '大胜',\n",
              " '军工厂',\n",
              " '克尽职守',\n",
              " '纵身',\n",
              " '进货关',\n",
              " '送别',\n",
              " '邮电部',\n",
              " '拐',\n",
              " '农区',\n",
              " '胡乱',\n",
              " '棉籽饼',\n",
              " '小鬼',\n",
              " '拍卖行',\n",
              " '接访',\n",
              " '水温',\n",
              " '海涂',\n",
              " '科罗尔',\n",
              " '硅石',\n",
              " '统一战线',\n",
              " '全国妇联',\n",
              " '肩胛骨',\n",
              " '船速',\n",
              " '转引',\n",
              " '弄权',\n",
              " '试射',\n",
              " '豆油',\n",
              " '酱坊',\n",
              " '词汇表']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CqvBHltvsonQ"
      },
      "source": [
        "完全切分"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JcTbNxFKsDol",
        "outputId": "d2343529-955f-4879-8548-23da58c25f24"
      },
      "source": [
        "def fully_segment(text, dic):\r\n",
        "    word_list = []\r\n",
        "    for i in range(len(text)):                  # i 从 0 到text的最后一个字的下标遍历\r\n",
        "        for j in range(i + 1, len(text) + 1):   # j 遍历[i + 1, len(text)]区间\r\n",
        "            word = text[i:j]                    # 取出连续区间[i, j]对应的字符串\r\n",
        "            if word in dic:                     # 如果在词典中，则认为是一个词\r\n",
        "                word_list.append(word)\r\n",
        "    return word_list\r\n",
        "  \r\n",
        "dic = load_dictionary()\r\n",
        "print(fully_segment('就读北京大学', dic))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['就', '就读', '读', '北', '北京', '北京大学', '京', '大', '大学', '学']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uowiAh-IsxYW"
      },
      "source": [
        "正向最长匹配"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "COVPbKcFURRX",
        "outputId": "0592981c-653b-4774-ac13-60e5a0edc1a1"
      },
      "source": [
        "def forward_segment(text, dic):\r\n",
        "    word_list = []\r\n",
        "    i = 0\r\n",
        "    while i < len(text):\r\n",
        "        longest_word = text[i]                      # 当前扫描位置的单字\r\n",
        "        for j in range(i + 1, len(text) + 1):       # 所有可能的结尾\r\n",
        "            word = text[i:j]                        # 从当前位置到结尾的连续字符串\r\n",
        "            if word in dic:                         # 在词典中\r\n",
        "                if len(word) > len(longest_word):   # 并且更长\r\n",
        "                    longest_word = word             # 则更优先输出\r\n",
        "        word_list.append(longest_word)              # 输出最长词\r\n",
        "        i += len(longest_word)                      # 正向扫描\r\n",
        "    return word_list\r\n",
        "\r\n",
        "dic = load_dictionary()\r\n",
        "print(forward_segment('就读北京大学', dic))\r\n",
        "print(forward_segment('研究生命起源', dic))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['就读', '北京大学']\n",
            "['研究生', '命', '起源']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EgSWR6NJs7UV"
      },
      "source": [
        "逆向最长匹配"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kKb2Ul3Gs76i",
        "outputId": "eadddf29-55e8-4df1-b434-46031e431876"
      },
      "source": [
        "def backward_segment(text, dic):\r\n",
        "    word_list = []\r\n",
        "    i = len(text) - 1\r\n",
        "    while i >= 0:                                   # 扫描位置作为终点\r\n",
        "        longest_word = text[i]                      # 扫描位置的单字\r\n",
        "        for j in range(0, i):                       # 遍历[0, i]区间作为待查询词语的起点\r\n",
        "            word = text[j: i + 1]                   # 取出[j, i]区间作为待查询单词\r\n",
        "            if word in dic:\r\n",
        "                if len(word) > len(longest_word):   # 越长优先级越高\r\n",
        "                    longest_word = word\r\n",
        "                    break\r\n",
        "        word_list.insert(0, longest_word)           # 逆向扫描，所以越先查出的单词在位置上越靠后\r\n",
        "        i -= len(longest_word)\r\n",
        "    return word_list\r\n",
        "\r\n",
        "dic = load_dictionary()\r\n",
        "print(backward_segment('研究生命起源', dic))\r\n",
        "print(backward_segment('项目的研究', dic))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['研究', '生命', '起源']\n",
            "['项', '目的', '研究']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MSsVO03otbeA"
      },
      "source": [
        "双向最长匹配"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HmjAqWgQtfd8",
        "outputId": "74ad26e6-9315-4aa6-c09f-08854a5962c1"
      },
      "source": [
        "def count_single_char(word_list: list):  # 统计单字成词的个数\r\n",
        "    return sum(1 for word in word_list if len(word) == 1)\r\n",
        "\r\n",
        "\r\n",
        "def bidirectional_segment(text, dic):\r\n",
        "    f = forward_segment(text, dic)\r\n",
        "    b = backward_segment(text, dic)\r\n",
        "    if len(f) < len(b):                                  # 词数更少优先级更高\r\n",
        "        return f\r\n",
        "    elif len(f) > len(b):\r\n",
        "        return b\r\n",
        "    else:\r\n",
        "        if count_single_char(f) < count_single_char(b):  # 单字更少优先级更高\r\n",
        "            return f\r\n",
        "        else:\r\n",
        "            return b                                     # 都相等时逆向匹配优先级更高\r\n",
        "        \r\n",
        "\r\n",
        "print(bidirectional_segment('研究生命起源', dic))\r\n",
        "print(bidirectional_segment('项目的研究', dic))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['研究', '生命', '起源']\n",
            "['项', '目的', '研究']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A4Q8cOa8v6m8"
      },
      "source": [
        "# 字典树与HanLP词典分词实现"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WO-ZxVqvv8jk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3621c56f-a1c1-48bf-a4a5-25066f845ee3"
      },
      "source": [
        "\r\n",
        "## 节点类 \r\n",
        "class Node():\r\n",
        "    def __init__(self) -> None:\r\n",
        "        self.children = {}\r\n",
        "        self.value = None\r\n",
        "    \r\n",
        "    # 增加节点\r\n",
        "    def add_child(self, char, value, overwrite=False):\r\n",
        "        child = self.children.get(char)\r\n",
        "        if child is None:\r\n",
        "            child = Node()                # 创建子节点\r\n",
        "            self.children[char] = child   # 子节点赋值，字 -> 节点的映射\r\n",
        "        \r\n",
        "        if value is not None or overwrite:\r\n",
        "            child.value = value           # 节点上对应的词\r\n",
        "        \r\n",
        "        return child\r\n",
        "    \r\n",
        "## 字典树  继承节点类   \r\n",
        "class Trie(Node):\r\n",
        "\r\n",
        "    def __contains__(self, key):\r\n",
        "        return self[key] is not None\r\n",
        "    \r\n",
        "    # 查询方法\r\n",
        "    def __getitem__(self, key):\r\n",
        "        state = self\r\n",
        "        for char in key:\r\n",
        "            state = state.children.get(char)\r\n",
        "            if state is None:\r\n",
        "                return None\r\n",
        "        \r\n",
        "        return state.value\r\n",
        "    \r\n",
        "    # 重载方法，使得类可以像对待dict那样操作字典树\r\n",
        "    # 构建一个词的字典树\r\n",
        "    def __setitem__(self, key, value):\r\n",
        "        state = self\r\n",
        "        for i, char in enumerate(key):\r\n",
        "            if i < len(key) - 1:\r\n",
        "                state = state.add_child(char, None)\r\n",
        "            else:\r\n",
        "                state = state.add_child(char, value, True)\r\n",
        "\r\n",
        "\r\n",
        "if __name__ == '__main__':\r\n",
        "    trie = Trie()\r\n",
        "    # 增\r\n",
        "    trie['自然'] = 'nature'\r\n",
        "    trie['自然人'] = 'human'\r\n",
        "    trie['自然语言'] = 'language'\r\n",
        "    trie['自语'] = 'talk\tto oneself'\r\n",
        "    trie['入门'] = 'introduction'\r\n",
        "    assert '自然' in trie\r\n",
        "    # 删\r\n",
        "    trie['自然'] = None\r\n",
        "    assert '自然' not in trie\r\n",
        "    # 改\r\n",
        "    trie['自然语言'] = 'human language'\r\n",
        "    assert trie['自然语言'] == 'human language'\r\n",
        "    # 查\r\n",
        "    assert trie['入门'] == 'introduction'\r\n",
        "    print()\r\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZB2qRJjwC_5"
      },
      "source": [
        "HanLP的词典分词实现"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HblTZpmTwH5x",
        "outputId": "4c87a3e5-9ffc-4a87-cb1b-d201a365d84f"
      },
      "source": [
        "# pip install pyhanlp"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyhanlp\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8f/99/13078d71bc9f77705a29f932359046abac3001335ea1d21e91120b200b21/pyhanlp-0.1.66.tar.gz (86kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 7.8MB/s \n",
            "\u001b[?25hCollecting jpype1==0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/07/09/e19ce27d41d4f66d73ac5b6c6a188c51b506f56c7bfbe6c1491db2d15995/JPype1-0.7.0-cp36-cp36m-manylinux2010_x86_64.whl (2.7MB)\n",
            "\u001b[K     |████████████████████████████████| 2.7MB 11.8MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyhanlp\n",
            "  Building wheel for pyhanlp (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyhanlp: filename=pyhanlp-0.1.66-py2.py3-none-any.whl size=29371 sha256=050aa3296b50fbe698d63a084b943b503b541d572ccc25a360e305d18d2f6159\n",
            "  Stored in directory: /root/.cache/pip/wheels/25/8d/5d/6b642484b1abd87474914e6cf0d3f3a15d8f2653e15ff60f9e\n",
            "Successfully built pyhanlp\n",
            "Installing collected packages: jpype1, pyhanlp\n",
            "Successfully installed jpype1-0.7.0 pyhanlp-0.1.66\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        },
        "id": "2omR7xIZwEA1",
        "outputId": "ae2115f2-a378-4936-c6cf-1c250fc3e45b"
      },
      "source": [
        "from pyhanlp import *\r\n",
        "\r\n",
        "# 不显示词性\r\n",
        "HanLP.Config.ShowTermNature = False\r\n",
        "\r\n",
        "# 可传入自定义字典 [dir1, dir2]\r\n",
        "segment = DoubleArrayTrieSegment()\r\n",
        "# 激活数字和英文识别\r\n",
        "segment.enablePartOfSpeechTagging(True)\r\n",
        "\r\n",
        "print(segment.seg(\"江西鄱阳湖干枯，中国最大淡水湖变成大草原\"))\r\n",
        "print(segment.seg(\"上海市虹口区大连西路550号SISU\"))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "error",
          "ename": "java.lang.IndexOutOfBoundsException",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mjava.lang.IndexOutOfBoundsException\u001b[0m       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-7105df8b9599>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# 可传入自定义字典 [dir1, dir2]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0msegment\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDoubleArrayTrieSegment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;31m# 激活数字和英文识别\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0msegment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menablePartOfSpeechTagging\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyhanlp/__init__.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    171\u001b[0m             \u001b[0mproxy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_proxy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m             \u001b[0mproxy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_proxy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mSafeJClass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproxy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/jpype/_jobject.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__javavalue__'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__javavalue__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m             \u001b[0mjv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__javaclass__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewInstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m             \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__javavalue__'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mJObject\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mjava.lang.IndexOutOfBoundsException\u001b[0m: Index 0 out of bounds for length 0"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BPGBA2dMxabZ"
      },
      "source": [
        "去掉停用词"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "id": "2wS6KHdVxa-C",
        "outputId": "efa4b866-04fb-4d58-f0cb-3590ad971d49"
      },
      "source": [
        "def load_from_file(path):\r\n",
        "    \"\"\"\r\n",
        "    从词典文件加载DoubleArrayTrie\r\n",
        "    :param path: 词典路径\r\n",
        "    :return: 双数组trie树\r\n",
        "    \"\"\"\r\n",
        "    map = JClass('java.util.TreeMap')()  # 创建TreeMap实例\r\n",
        "    with open(path) as src:\r\n",
        "        for word in src:\r\n",
        "            word = word.strip()  # 去掉Python读入的\\n\r\n",
        "            map[word] = word\r\n",
        "    return JClass('com.hankcs.hanlp.collection.trie.DoubleArrayTrie')(map)\r\n",
        "\r\n",
        "\r\n",
        "## 去掉停用词\r\n",
        "def remove_stopwords_termlist(termlist, trie):\r\n",
        "    return [term.word for term in termlist if not trie.containsKey(term.word)]\r\n",
        "\r\n",
        "\r\n",
        "trie = load_from_file('stopwords.txt')\r\n",
        "termlist = segment.seg(\"江西鄱阳湖干枯了，中国最大的淡水湖变成了大草原\")\r\n",
        "print('去掉停用词前：', termlist)\r\n",
        "\r\n",
        "print('去掉停用词后：', remove_stopwords_termlist(termlist, trie))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-56472af8cfe0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mtrie\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_from_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'stopwords.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mtermlist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msegment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"江西鄱阳湖干枯了，中国最大的淡水湖变成了大草原\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'去掉停用词前：'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtermlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'segment' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucTmCseHx5Pu"
      },
      "source": [
        "# 二元语法与中文分词"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tv0pe756y3jx"
      },
      "source": [
        "训练与预测"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w3moCX_oyhD1"
      },
      "source": [
        "from pyhanlp import *\r\n",
        "from jpype import JString\r\n",
        "\r\n",
        "## 加载 JAVA 类\r\n",
        "CorpusLoader = SafeJClass('com.hankcs.hanlp.corpus.document.CorpusLoader')\r\n",
        "NatureDictionaryMaker = SafeJClass('com.hankcs.hanlp.corpus.dictionary.NatureDictionaryMaker')\r\n",
        "CoreDictionary = LazyLoadingJClass('com.hankcs.hanlp.dictionary.CoreDictionary')\r\n",
        "WordNet = JClass('com.hankcs.hanlp.seg.common.WordNet')\r\n",
        "Vertex = JClass('com.hankcs.hanlp.seg.common.Vertex')\r\n",
        "\r\n",
        "\r\n",
        "def generate_wordnet(sent, trie):\r\n",
        "    \"\"\"\r\n",
        "    生成词网\r\n",
        "    :param sent: 句子\r\n",
        "    :param trie: 词典（unigram）\r\n",
        "    :return: 词网\r\n",
        "    \"\"\"\r\n",
        "    searcher = trie.getSearcher(JString(sent), 0)\r\n",
        "    wordnet = WordNet(sent)\r\n",
        "    while searcher.next():\r\n",
        "        wordnet.add(searcher.begin + 1,\r\n",
        "                    Vertex(sent[searcher.begin:searcher.begin + searcher.length], searcher.value, searcher.index))\r\n",
        "    # 原子分词，保证图连通\r\n",
        "    vertexes = wordnet.getVertexes()\r\n",
        "    i = 0\r\n",
        "    while i < len(vertexes):\r\n",
        "        if len(vertexes[i]) == 0:  # 空白行\r\n",
        "            j = i + 1\r\n",
        "            for j in range(i + 1, len(vertexes) - 1):  # 寻找第一个非空行 j\r\n",
        "                if len(vertexes[j]):\r\n",
        "                    break\r\n",
        "            wordnet.add(i, Vertex.newPunctuationInstance(sent[i - 1: j - 1]))  # 填充[i, j)之间的空白行\r\n",
        "            i = j\r\n",
        "        else:\r\n",
        "            i += len(vertexes[i][-1].realWord)\r\n",
        "\r\n",
        "    return wordnet\r\n",
        "\r\n",
        "\r\n",
        "## 维特比算法\r\n",
        "def viterbi(wordnet):\r\n",
        "    nodes = wordnet.getVertexes()\r\n",
        "    # 前向遍历\r\n",
        "    for i in range(0, len(nodes) - 1):\r\n",
        "        for node in nodes[i]:\r\n",
        "            for to in nodes[i + len(node.realWord)]:\r\n",
        "                to.updateFrom(node)  # 根据距离公式计算节点距离，并维护最短路径上的前驱指针from\r\n",
        "    # 后向回溯\r\n",
        "    path = []  # 最短路径\r\n",
        "    f = nodes[len(nodes) - 1].getFirst()  # 从终点回溯\r\n",
        "    while f:\r\n",
        "        path.insert(0, f)\r\n",
        "        f = f.getFrom()  # 按前驱指针from回溯\r\n",
        "    return [v.realWord for v in path]\r\n",
        "\r\n",
        "\r\n",
        "## 训练n元语法模型\r\n",
        "def train_bigram(corpus_path, model_path):\r\n",
        "    sents = CorpusLoader.convert2SentenceList(corpus_path)\r\n",
        "    for sent in sents:\r\n",
        "        for word in sent:\r\n",
        "            word.setLabel(\"n\")\r\n",
        "    \r\n",
        "    maker = NatureDictionaryMaker()\r\n",
        "    maker.compute(sents)\r\n",
        "    maker.saveTxtTo(model_path)      # 会生成两个统计词频文件\r\n",
        "\r\n",
        "## 加载 n元语法模型\r\n",
        "def load_bigram(model_path, sent):\r\n",
        "    HanLP.Config.CoreDictionaryPath = model_path + \".txt\"  # unigram\r\n",
        "    HanLP.Config.BiGramDictionaryPath = model_path + \".ngram.txt\"  # bigram\r\n",
        "    \r\n",
        "    wordnet = generate_wordnet(sent, CoreDictionary.trie)\r\n",
        "    print(viterbi(wordnet))\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "corpus_path = 'my_cws_corpus.txt' # 语料库文件\r\n",
        "model_path = 'my_cws_model'       # 模型保存路径\r\n",
        "sent = '货币和服务'                 # 需要分词的语句\r\n",
        "\r\n",
        "train_bigram(corpus_path, model_path)\r\n",
        "load_bigram(model_path, sent)\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-EiIw8lyzZM-"
      },
      "source": [
        "HanLP分词器简洁版："
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fmm4nTeyzVO9"
      },
      "source": [
        "from pyhanlp import *\r\n",
        "\r\n",
        "ViterbiSegment = SafeJClass('com.hankcs.hanlp.seg.Viterbi.ViterbiSegment')\r\n",
        "\r\n",
        "segment = ViterbiSegment()\r\n",
        "sentence = \"社会摇摆简称社会摇\"\r\n",
        "segment.enableCustomDictionary(False)\r\n",
        "print(\"不挂载词典：\", segment.seg(sentence))\r\n",
        "CustomDictionary.insert(\"社会摇\", \"nz 100\")\r\n",
        "segment.enableCustomDictionary(True)\r\n",
        "print(\"低优先级词典：\", segment.seg(sentence))\r\n",
        "segment.enableCustomDictionaryForcing(True)\r\n",
        "print(\"高优先级词典：\", segment.seg(sentence))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQqJ87Pzzj-G"
      },
      "source": [
        "可见，用户词典的高优先级未必是件好事"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JU5016iTlmPr"
      },
      "source": [
        "# 隐马尔可夫模型与序列标注"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gti6OiZLl-iZ"
      },
      "source": [
        "## 隐马尔可夫模型的训练"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2rq3HhO9mWA9"
      },
      "source": [
        "案例假设和模型构造"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L7ESYbUjmJtz"
      },
      "source": [
        "import numpy as np\r\n",
        "from pyhanlp import *\r\n",
        "from jpype import JArray, JFloat, JInt\r\n",
        "\r\n",
        "to_str = JClass('java.util.Arrays').toString\r\n",
        "\r\n",
        "## 隐马尔可夫模型描述\r\n",
        "states = ('Healthy', 'Fever')\r\n",
        "start_probability = {'Healthy': 0.6, 'Fever': 0.4}\r\n",
        "transition_probability = {\r\n",
        "    'Healthy': {'Healthy': 0.7, 'Fever': 0.3},\r\n",
        "    'Fever': {'Healthy': 0.4, 'Fever': 0.6},\r\n",
        "}\r\n",
        "emission_probability = {\r\n",
        "    'Healthy': {'normal': 0.5, 'cold': 0.4, 'dizzy': 0.1},\r\n",
        "    'Fever': {'normal': 0.1, 'cold': 0.3, 'dizzy': 0.6},\r\n",
        "}\r\n",
        "observations = ('normal', 'cold', 'dizzy')\r\n",
        "\r\n",
        "\r\n",
        "def generate_index_map(lables):\r\n",
        "    index_label = {}\r\n",
        "    label_index = {}\r\n",
        "    i = 0\r\n",
        "    for l in lables:\r\n",
        "        index_label[i] = l\r\n",
        "        label_index[l] = i\r\n",
        "        i += 1\r\n",
        "    return label_index, index_label\r\n",
        "\r\n",
        "\r\n",
        "states_label_index, states_index_label = generate_index_map(states)\r\n",
        "observations_label_index, observations_index_label = generate_index_map(observations)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "def convert_map_to_matrix(map, label_index1, label_index2):\r\n",
        "    m = np.empty((len(label_index1), len(label_index2)), dtype=float)\r\n",
        "    for line in map:\r\n",
        "        for col in map[line]:\r\n",
        "            m[label_index1[line]][label_index2[col]] = map[line][col]\r\n",
        "    return JArray(JFloat, m.ndim)(m.tolist())\r\n",
        "\r\n",
        "def convert_observations_to_index(observations, label_index):\r\n",
        "    list = []\r\n",
        "    for o in observations:\r\n",
        "        list.append(label_index[o])\r\n",
        "    return list\r\n",
        "\r\n",
        "def convert_map_to_vector(map, label_index):\r\n",
        "    v = np.empty(len(map), dtype=float)\r\n",
        "    for e in map:\r\n",
        "        v[label_index[e]] = map[e]\r\n",
        "    return JArray(JFloat, v.ndim)(v.tolist())  # 将numpy数组转为Java数组\r\n",
        "\r\n",
        "\r\n",
        "## pi：初始状态概率向量\r\n",
        "## A：状态转移概率矩阵\r\n",
        "## B：发射概率矩阵\r\n",
        "A = convert_map_to_matrix(transition_probability, states_label_index, states_label_index)\r\n",
        "B = convert_map_to_matrix(emission_probability, states_label_index, observations_label_index)\r\n",
        "observations_index = convert_observations_to_index(observations, observations_label_index)\r\n",
        "pi = convert_map_to_vector(start_probability, states_label_index)\r\n",
        "\r\n",
        "FirstOrderHiddenMarkovModel = JClass('com.hankcs.hanlp.model.hmm.FirstOrderHiddenMarkovModel')\r\n",
        "given_model = FirstOrderHiddenMarkovModel(pi, A, B)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-gWOce9GmYL3"
      },
      "source": [
        "样本生成算法"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QmVPw4YhmZ2O",
        "outputId": "4f3cae16-0169-4b86-bdee-05ef626919dc"
      },
      "source": [
        "## 第一个参数：序列最低长度\r\n",
        "## 第二个参数：序列最高长度\r\n",
        "## 第三个参数：需要生成的样本数\r\n",
        "for O, S in given_model.generate(3, 5, 2):\r\n",
        "    print(\" \".join((observations_index_label[o] + '/' + states_index_label[s]) for o, s in zip(O, S)))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cold/Fever cold/Fever dizzy/Fever\n",
            "normal/Healthy cold/Healthy normal/Healthy cold/Healthy\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R4gzV54Imqh7"
      },
      "source": [
        "隐马尔可夫模型的训练"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Di4O80rZms6j",
        "outputId": "0fb081c2-f8a1-442f-87c0-f6a5c0a3e173"
      },
      "source": [
        "trained_model = FirstOrderHiddenMarkovModel()\r\n",
        "\r\n",
        "## 第一个参数：序列最低长度\r\n",
        "## 第二个参数：序列最高长度\r\n",
        "## 第三个参数：需要生成的样本数\r\n",
        "trained_model.train(given_model.generate(3, 10, 100000))\r\n",
        "print('新模型与旧模型是否相同：', trained_model.similar(given_model))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "新模型与旧模型是否相同： True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YMsI3_EjnAdI"
      },
      "source": [
        "## 隐马尔可夫模型的预测"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pwwaOadbnGf_"
      },
      "source": [
        "搜索状态序列的维特比算法"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9GGXqO4CnYO-",
        "outputId": "4bc459d6-e0de-4563-d60d-3369d566841a"
      },
      "source": [
        "pred = JArray(JInt, 1)([0, 0, 0])\r\n",
        "prob = given_model.predict(observations_index, pred)\r\n",
        "print(\" \".join((observations_index_label[o] + '/' + states_index_label[s]) for o, s in\r\n",
        "               zip(observations_index, pred)) + \" {:.3f}\".format(np.math.exp(prob)))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "normal/Healthy cold/Healthy dizzy/Fever 0.015\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}